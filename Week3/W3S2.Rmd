---
title: "W3S2"
author: "Hussain Alsalman"
date: "5/19/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("tidyverse")
library("here")
library("broom")
library("coefplot")
library("useful")
```

## What We learned so far?

 - 
 - 
 - 
 - 
 
## What you will learn tonight 

 - 
 - 
 - 
 
## Introduction to Logistic Regression 

Not all data can be appropriately modeled with linear regression, because they are **binomial** (TRUE/FALSE), count data or some other form. To model these types of data, we use different type linear models. Logestic Regression is one of them. 

## We can't we just use the simple regression line. 


Consider this example 

```{r}
#Why can't we just do regression model .. what can go wrong? 
iris[1:100,] %>%  mutate( class = ifelse(Species == "setosa",1,0)) %>% 
ggplot(mapping = aes(x = Sepal.Length, y = class, group  =1)) + geom_point()   + 
  stat_smooth(geom = "line",method= "lm", se = FALSE,alpha = 0.7, color = "navy",linetype = "dashed") + 
  
  #or we can alternatively use more flexible line   
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, color="orangered2")


```


## Logestic Regression Models 

One of the most powerfull tools to answer yes/no questions (_binary classificaiton_) is logestic regression.

remember this equaiton ? 

$$ 
y = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \dots +  \beta_{n}x_{n} + \epsilon
$$ 

we will make few modifications to make the output from 0 to 1. This modification is called **logit transofrmation**.

The final function will look something like this 

$$
\pi(x) = \frac{e^{\beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \dots +  \beta_{n}x_{n}}}{1 + e^{\beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \dots +  \beta_{n}x_{n}}}
$$


## Let's work by Example 

Here we have data for the households in New York State. 

### Can we Answer this question  : 
Can we classify whether a household has an income greater than $150,000 or not? 

```{r}
acs <- read.table("http://jaredlander.com/data/acs_ny.csv", sep=",", header=TRUE, stringsAsFactors=FALSE)

```


```{r}
# first thing we do is engineer a categorical variable for income 
library("dplyr")
library("tidyverse")

acs <- acs %>% mutate( Income = FamilyIncome >= 150000)

## Let's see visualize the income to get and idea about the distribution 

## to visualize how thie distribution income in the data 
library("ggplot2") 
ggplot(acs, mapping = aes(x = FamilyIncome)) + 
  geom_density(fill = "gray")   + 
  geom_vline(xintercept = 150000)
  


## We can now create our model simply but calling the glm function. `glm` stands for gereralized linear models. 

incom1 <- glm( Income ~ HouseCosts + NumWorkers + OwnRent + NumBedrooms + FamilyType, data = acs, family = "binomial") 

summary(incom1)

# we read the coefficients as weights and gain insight on about the relationship between independant variables and the depandent variable. 
## We can examine our betas if they are significant 

library("coefplot")

coefplot(incom1)

```


## How to select the variables for our regression model 

Our objective is to come up with a simple and powerful regression model. This model should have these two characteristics 

- Select only the important variables 
- Have stable weights _ coefficients_ 

$$
\hat{y} = \hat\beta_{0} + \hat\beta_{1}x_{1} + \hat\beta_{2}x_{2} + \dots +  \hat\beta_{n}x_{n}
$$

Remember that our objective is to minimize the sum of squared error **SSE**. 

$$
\sum_{i=1}^{n}(y_{i} - \hat{y}_i)^2
$$



We can add a penelty in the formula above for using too many variables. This is called **lasso regression**.  This penelty is also called **regularization**. 

We can also add another panelty for using big Betas.  This is called **ridge regression**. It is also called **shrinkage**. 

combining them together is called **Elastic Net**. 

The equation in the most simple and silly way would look like this 

$$
min(error) = SSE + panelty
$$

Real equatoin looks like this 

```{r, fig.align='center', echo = FALSE}
knitr::include_graphics(path = "https://www.analyticsvidhya.com/wp-content/uploads/2015/08/Elastic_Net.png")
```



## Math check !! 

Remember this equation. This is the same equation we have been seening all the time tonight but in different form. 

$$
\begin{bmatrix}
y_{1} \\ y_{2} \\ \dots \\ y_{n}
\end{bmatrix} =
\begin{bmatrix}
1 & x_{12} & x_{13} & \dots & x_{1n} \\
1 & x_{22} & x_{23} & \dots & x_{2n} \\
\dots  & \dots  & \dots  & \dots & \dots  \\
1 & x_{n2} & x_{n3} & \dots & x_{nn} 
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \dots \\ \beta_{n}
\end{bmatrix}
$$


The short form of this formula is 

$$
Y = X\beta
$$

Understanding this is important to go to the next step 


```{r}
testFrame <-
 data.frame(First=sample(1:10, 20, replace=TRUE),
 Second=sample(1:20, 20, replace=TRUE),
 Third=sample(1:10, 20, replace=TRUE),
 Fourth=factor(rep(c("Alice", "Bob", "Charlie", "David"), 5)),
 Fifth=ordered(rep(c("Edward", "Frank", "Georgia",
 "Hank", "Isaac"), 4)),
 Sixth=rep(c("a", "b"), 10), stringsAsFactors=F)

str(testFrame)
head(testFrame)

```

Let's design the X matrix. I hope you still remember how we encode the _categorical variables_ :) 

```{r}

head(model.matrix(First ~ Second + Fourth + Fifth, testFrame))


```

Notice that the `model.matrix` has converted the categorical variables to dummy variables. This is different with the `Fifth` column as this is an ordinal variable. 

for our purposes we need all variables as flags either _On_ or _Off_ . so we design it with modificaiton as the following 

```{r}
#we will use the build.x form 'useful' package 

head(build.x(First ~ Second + Fourth + Fifth, testFrame, contrasts = FALSE))

# we can remove the intercept by adding -1 

head(build.x(First ~ Second + Fourth + Fifth -1, testFrame, contrasts = FALSE))
```


## Let's go back to our house example.  

We will use `glment` package which implement this nice idea of panalizing the regression model. 


```{r}

#first we create the Design matrix X 
# do not include the intercept as glmnet will add that automatically
formula_m <- Income ~ NumBedrooms + NumChildren + NumPeople + NumRooms + NumUnits + NumVehicles + NumWorkers + OwnRent + YearBuilt + ElectricBill + FoodStamp + HeatingFuel + Insurance + Language - 1

acx_X <- build.x(formula_m, data = acs, contrasts = FALSE, sparse = TRUE)

#first we create the response matrix Y 
# do not include the intercept as glmnet will add that automatically
acy_Y <- build.y(formula_m,data = acs)


library("glmnet")
set.seed(1863561)

m1  <- glmnet(x = acx_X, y = acy_Y, family = "binomial")

##viewing the weights using different lambdas 

View(as.matrix(m1$beta))

```


Remember lambda? it is our penelty factor .. We decide the mix between lasso & rigid panelties by specifying `alpha`. the default value is 1 which is Lasso panelty. We can change it to .4 for example. This will mean that 40% lasso and 60% rigid.  

Since we have not specified lambda, the function makes 100 iterations with different lamdba as seen with `View(as.matrix(m1$beta))`

We can visualize the effect of choosing different lambda

```{r}
plot(m1, xvar = "lambda")

#We can see the effect of lambda interactively 
coefpath(m1)
```

#


