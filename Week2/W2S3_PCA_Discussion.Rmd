---
title: "W2S3_PCA_Discussion"
author: "Hussain Alsalman"
date: "5/18/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("tidyverse")
library("ggplot2")
library("here")
```


## Students Grades Dataset 

```{r}
std_grad <- tribble(
  ~math , ~history , ~arabic, ~chemistry,      
  10,         6,       12,     5,   
  11,         4,       9,      7,  
  8,          5,       10,     6,
  3,          3,       2.5,    2,     
  2,          2.8,     1.3,    4,
  1,          2,       2,      7
)
  
std_grad
```

## One Dimension 

If we only measure the variability of the data based on the math scores we can divide the data as following 

```{r}
g <- ggplot(std_grad, mapping = aes(y = 0 ,x = math)) + geom_text(aes( label = math)) + geom_point(size = 10, alpha =0.3, fill = "darkgreen")
g
```


We can see from the graph that math divides the data into two groups. "Dawafeer" and "Lazy"

```{r}
g + geom_point(aes(color= math > 6 ), show.legend = FALSE)
```



## Two Dimensions 

Let's now try to see the data into two dimensions 

```{r}
ggplot(std_grad, mapping = aes(y = history,x = math)) + geom_point(size = 10, alpha =0.3, fill = "darkgreen")
```


We can see from the graph that history and matth still divide the data into two groups. "Dawafeer" and "Lazy"


## Three Dimension 

```{r}

ggplot(std_grad, mapping = aes(y = history,x = math, size = arabic)) + geom_point( alpha =0.3, fill = "darkgreen")

```


With 3 dimensions we can still see the division between the groups. If we add fourth dimension it will be very difficult to visualize the data. 

Would it be great to reduce the dimensions and still keep the variance as much as possible between the data. We can then visualize them as well. 

This is what PCA does. Awesome !! 

## PCA outputs 

Before we get to know PCA what is the outcome from this analysis. In other words, why do we care about this complicated topic. 

We get three things from PCA 

- It tells use what points are related to each other _it helps create latent variables_
- It tells us what variables that are most important
- It tells us how accurate the visualization is 

so is this cool? do you agree that those things are actually awesome?.. I do :) 

## How does PCA work? 

let's go back to only two variables for now 

We draw project each point on its axis. like this 

```{r}
g1 <- ggplot(std_grad, mapping = aes(y = history,x = math)) + geom_point(size = 7, alpha =0.3, fill = "darkgreen")
g1

g2 <- g1+ geom_segment(aes(y = history,x=math,yend=0,xend =math),alpha = 0.4,arrow = arrow(length = unit(0.6,"cm")) , linetype = "dashed", color = "darkgreen", lineend = "round")
g2
```

Then we will calculate the mean for those projected points 

```{r}
g2.1 <- g2 + geom_point(aes(y = 0 , x = mean(math)),size = 5, color = "red", shape = 4) 
g2.1
```

we do the same thing for the history axis 

```{r}
g3 <- g2.1 + geom_segment(aes(x=math, y = history, yend = history, xend = 0),alpha= 0.4,arrow = arrow(length = unit(0.6,"cm")) , linetype = "dashed", color = "darkblue", lineend = "round")
g3
```

```{r}
g3.1 <- g3 + geom_point(aes(x = 0 , y = mean(history)),size = 5, color = "red", shape = 4) 
g3.1
```

```{r}
g4 <- g3.1 + 
  geom_segment(aes(x=mean(math), y = 0, yend = mean(history), xend = mean(math)),alpha= 0.8,arrow = arrow(length = unit(0.6,"cm")) , linetype = "solid", color = "navy", lineend = "round") +
  
  geom_segment(aes(x=0, y = mean(history), yend = mean(history), xend = mean(math)),alpha= 0.8,arrow = arrow(length = unit(0.6,"cm")) , linetype = "solid", color = "navy", lineend = "round") + 
  
   geom_point(aes(x = mean(math) , y = mean(history)),size = 10, color = "red", shape = 4)

g4
```


# Transforming the data

Now we transform the data and try to keep the distances the same. We will only change how we look at our data. 

we bring the center point to the origin 

```{r}
std_grad_trans <- std_grad %>%  mutate( math_trans = math - mean(math)) %>% mutate(history_trans = history - mean(history)) 

# We center the points at the origin point 
g5 <- std_grad_trans %>%  
  ggplot(mapping = aes(x = math_trans, y = history_trans)) + geom_point(size = 10, alpha =0.3, fill = "darkgreen") + 
  
  # draw the center point 
  geom_point(aes(x = 0 , y = 0),size = 10, color = "red", shape = 4) +
  
  # we try to fit a line that maximize the variance between the projected points 
  stat_smooth(geom='line', method = "lm", alpha=0.5,linetype = "dashed", color = "red", se=FALSE) 

g5
# User GoToMeeting dawing tools to show the project points 

```



# Welcome your First component Analysis PC1 

The line above is the first component analysis. We find this line by looking at many lines and pick the one that maximize the variance (the distance) or _eigen values_ . This is how we do it 


## Your first Eigen value

Eigen values are just the sum of all the squared distances between the origin and the projected points 

```{r, out.width='40%', fig.align='center'}
knitr::include_graphics(path = here("Images","PCA.png"))
```


The red Xs are the projected points. we measure the distances and we square them and sum them up 

```{r, out.width='40%', fig.align='center'}
knitr::include_graphics(path = here("Images","PCA_projected.png"))
```


let's see the relationship between between eigen values and the PC1 

```{r, out.width='40%', fig.align='center'}
knitr::include_graphics(path = here("Images","PCA_best_fit.gif"))
```


Now you know what Eigen values are, let me introduce you to Eigen vectors 


# Please Welcome your first Eigen Vector 


```{r}
slope_PC1 = coef(lm(data=std_grad_trans, history_trans~math_trans))[2] 

# Let's draw the original Y axis and X axis 
g5 + geom_vline(mapping = aes(xintercept = 0)) + geom_hline(mapping = aes(yintercept = 0)) +
  
  # The slope of this PC is 0.2910369 
  #let's see how is This means for every 1 unit in History there is 3.5 in math. This PC1 is like a cocktil of H 1: M 3.5
  geom_segment(mapping = aes(y = 0, yend = 0 , x = 0, xend= 4), arrow = arrow(length = unit(0.6,"cm")) ) + 
  geom_segment(mapping = aes(y = 0, yend = 1 , x = 4, xend=4), arrow = arrow(length = unit(0.6,"cm"))) + 
  geom_text(mapping = aes(y = 1 , x = 4 , label = 1), nudge_x = 0.3, nudge_y = -.5 ) + 
  geom_text(mapping = aes(y = 0 , x = 4 , label = 4), nudge_y = -0.2, nudge_x = -1.3 ) +
  
  
  #lets calculate the hypotenuse 
  geom_text(mapping = aes(y = 0 , x = 1.2 , label = 4.12, nudge_y = 0.4)
  
```


```{r}
#eigen vector 

3.43/(3.573)

1/(3.573)

# In r there one magical function can do all this for us and for all Principle component Analysises. 
prcomp(std_grad_trans[,1:2]) 
```

